## OpenShift Infrastructure Nodes
The OpenShift subscription model allows customers to run various core
infrastructure components at no additional charge. In other words, a node
that is only running core OpenShift infrastructure components is not counted
in terms of the total number of subscriptions required to cover the
environment.

OpenShift components that fall into the infrastructure categorization
include:

* kubernetes and OpenShift control plane services ("masters")
* router
* container image registry
* cluster metrics collection ("monitoring")
* cluster aggregated logging
* service brokers

Any node running a container/pod/component not described above is considered
a worker and must be covered by a subscription.

### More MachineSet Details
In the `MachineSets` exercises you explored using `MachineSets` and scaling
the cluster by changing their replica count. In the case of an infrastructure
node, we want to create additional `Machines` that have specific Kubernetes
labels. Then, we can configure the various infrastructure components to run
specifically on nodes with those labels.

[NOTE]
====
Currently the operators that are used to control infrastructure components do
not all support the use of taints and tolerations. This means that
infrastructure workload will go onto the infrastructure nodes, but other
workload is not specifically prevented from running on the infrastructure
nodes. In other words, user workload may commingle with infrastructure
workload until full taint/toleration support is implemented in all operators.

The use of taints/tolerations is not covered in any of these exercises.
====

To accomplish this, you will create additional `MachineSets`.

In order to understand how `MachineSets` work, run the following.

This will allow you to follow along with some of the following discussion.

[source,bash,role="copypaste copypaste-warning"]
----
oc get machineset -n openshift-machine-api -o yaml cluster-5fa6-hx2ml-worker-us-east-2c
----

#### Metadata
The `metadata` on the `MachineSet` itself includes information like the name
of the `MachineSet` and various labels.

```YAML
metadata:
  creationTimestamp: 2019-01-25T16:00:34Z
  generation: 1
  labels:
    machine.openshift.io/cluster-api-cluster: 190125-3
    machine.openshift.io/cluster-api-machine-role: worker
    machine.openshift.io/cluster-api-machine-type: worker
  name: 190125-3-worker-us-east-1b
  namespace: openshift-machine-api
  resourceVersion: "9027"
  selfLink: /apis/cluster.k8s.io/v1alpha1/namespaces/openshift-machine-api/machinesets/190125-3-worker-us-east-1b
  uid: 591b4d06-20ba-11e9-a880-068acb199400
```

[NOTE]
====
You might see some `annotations` on your `MachineSet` if you dumped
one that had a `MachineAutoScaler` defined.
====

#### Selector
The `MachineSet` defines how to create `Machines`, and the `Selector` tells
the operator which machines are associated with the set:

```YAML
spec:
  replicas: 2
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: 190125-3
      machine.openshift.io/cluster-api-machineset: 190125-3-worker-us-east-1b
```

In this case, the cluster name is `190125-3` and there is an additional
label for the whole set.

### Template Metadata
The `template` is the part of the `MachineSet` that templates out the
`Machine`. The `template` itself can have metadata associated, and we need to
make sure that things match here when we make changes:

```YAML
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: 190125-3
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: 190125-3-worker-us-east-1b
```

#### Template Spec
The `template` needs to specify how the `Machine`/`Node` should be created.
You will notice that the `spec` and, more specifically, the `providerSpec`
contains all of the important AWS data to help get the `Machine` created
correctly and bootstrapped.

In our case, we want to ensure that the resulting node inherits one or more
specific labels. As you've seen in the examples above, labels go in
`metadata` sections:

```YAML
  spec:
      metadata:
        creationTimestamp: null
      providerSpec:
        value:
          ami:
            id: ami-08871aee06d13e584
...
```

By default the `MachineSets` that the installer creates do not apply any
additional labels to the node.

### Defining a Custom MachineSet
Now that you've analyzed an existing `MachineSet` it's time to go over the
rules for creating one, at least for a simple change like we're making:

1. Don't change anything in the `providerSpec`
2. Don't change any instances of `machine.openshift.io/cluster-api-cluster: <clusterid>`
3. Give your `MachineSet` a unique `name`
4. Make sure any instances of `machine.openshift.io/cluster-api-machineset` match the `name`
5. Add labels you want on the nodes to `.spec.template.spec.metadata.labels`
6. Even though you're changing `MachineSet` `name` references, be sure not to change the `subnet`.

This sounds complicated, but we have a little program and some steps that
will do the hard work for you:

[source,bash,role="execute"]
----
bash $HOME/support/machineset-generator.sh 1 infra 0 | oc create -f -
export MACHINESET=$(oc get machineset -n openshift-machine-api -l machine.openshift.io/cluster-api-machine-role=infra -o jsonpath='{.items[0].metadata.name}')
oc patch machineset $MACHINESET -n openshift-machine-api --type='json' -p='[{"op": "add", "path": "/spec/template/spec/metadata/labels", "value":{"node-role.kubernetes.io/worker":"", "node-role.kubernetes.io/infra":""} }]'
oc scale machineset $MACHINESET -n openshift-machine-api --replicas=1
----

Then go ahead and run:
[source,bash,role="execute"]
----
oc get machineset -n openshift-machine-api
----

You should see the new infra set listed with a name similar to the following:

```
...
cluster-city-56f8-mc4pf-infra-us-east-2a    1         1                             13s
...
```

We don't yet have any ready or available machines in the set because the
instances are still coming up and bootstrapping. You can check `oc get
machine -n openshift-machine-api` to see when the instance finally starts
running. Then, you can use `oc get node` to see when the actual node is
joined and ready.

[NOTE]
====
It can take several minutes for a `Machine` to be prepared and added as a `Node`.
====

[source,bash,role="execute"]
----
oc get nodes
----

```
NAME                                         STATUS   ROLES          AGE     VERSION
ip-10-0-133-134.us-east-2.compute.internal   Ready    infra,worker   8m     v1.16.2
ip-10-0-133-191.us-east-2.compute.internal   Ready    worker         61m    v1.16.2
ip-10-0-136-83.us-east-2.compute.internal    Ready    master         67m    v1.16.2
ip-10-0-138-24.us-east-2.compute.internal    Ready    infra,worker   8m1s   v1.16.2
ip-10-0-139-81.us-east-2.compute.internal    Ready    infra,worker   8m3s   v1.16.2
ip-10-0-152-132.us-east-2.compute.internal   Ready    worker         61m    v1.16.2
ip-10-0-157-139.us-east-2.compute.internal   Ready    master         67m    v1.16.2
ip-10-0-167-9.us-east-2.compute.internal     Ready    worker         61m    v1.16.2
ip-10-0-169-121.us-east-2.compute.internal   Ready    master         67m    v1.16.2
```

If you're having trouble figuring out which node is the new
one, take a look at the `AGE` column. It will be the youngest! Also, in the
`ROLES` column you will notice that the new node has both a `worker` and an
`infra` role.

### Check the Labels
In our case, the youngest node was named
`ip-10-0-128-138.us-east-1.compute.internal`, so we can ask what its labels
are:

[source,bash,role="copypaste copypaste-warning"]
----
oc get node ip-10-0-139-81.us-east-2.compute.internal --show-labels
----

And, in the `LABELS` column we see:

    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2a,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-0-140-3,kubernetes.io/os=linux,node-role.kubernetes.io/infra=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos

It's hard to see, but our `node-role.kubernetes.io/infra` label is there.

### Add More Machinesets (or scale, or both)
In a realistic production deployment, you would want at least 3 `MachineSets`
to hold infrastructure components. Both the logging aggregation solution and
the service mesh will deploy ElasticSearch, and ElasticSearch really needs 3
instances spread across 3 discrete nodes. Why 3 `MachineSets`? Well, in
theory, having multiple `MachineSets` in different AZs ensures that you don't
go completely dark if AWS loses an AZ.

The `MachineSet` you created with the scriptlet already created 3 replicas
for you, so you don't have to do anything for now. Don't create any
additional ones yourself, either -- the AWS limits on the account you are
using are purposefully small.

### Extra Credit
In the `openshift-machine-api` project are several `Pods`. One of them has a
name like `machine-api-controllers-56bdc6874f-86jnb`. If you use `oc logs` on the
various containers in that `Pod`, you will see the various operator bits that
actually make the nodes come into existence.

## Quick Operator Background
Operators are just `Pods`. But they are special `Pods`. They are software
that understands how to deploy and manage applications in a Kubernetes
environment. The power of Operators relies on a recent Kubernetes feature
called `CustomResourceDefinitions` (`CRD`). A `CRD` is exactly what it sounds
like. They are a way to define a custom resource which is essentially
extending the Kubernetes API with new objects.

If you wanted to be able to create/read/update/delete `Foo` objects in
Kubernetes, you would create a `CRD` that defines what a `Foo` resource is and how it
works. You can then create `CustomResources` (`CRs`) -- instances of your `CRD`.

With Operators, the general pattern is that an Operator looks at `CRs` for its
configuration, and then it _operates_ on the Kubernetes environment to do
whatever the configuration specifies. Now you will take a look at how some of
the infrastructure operators in OpenShift do their thing.
